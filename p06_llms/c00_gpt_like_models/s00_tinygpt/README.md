### Introduction
This is an example of a simple workflow training a tiny GPT-like model, demonstrating
tokenisation, encoding, embeddings, and training a model using PyTorch based on work by 
[codewithaarohi](https://github.com/codewithaarohi) (see acknowledgements at the end of the file).
It is tiny because the training data is very, very small and the model is very simple, the encoding is limited, but it
demonstrates the core ideas of training a language model using PyTorch.

### Instructions:
 1. Install the environment gzh_llm0 from conda_env_gzh_llm0.yml with conda and activate it
 2. Run python main.py to execute the code (current directory as working directory)

## Acknowledgement
This repository is a fork of https://github.com/codewithaarohi/Build-a-Mini-GPT-Model-From-Scratch-Using-PyTorch.

Non-material modifications were made to the original code, including minor formatting changes, renaming of files.
